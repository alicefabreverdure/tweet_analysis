2022-12-19 11:12:13,463 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,467 Model: "TextClassifier(
  (decoder): Linear(in_features=512, out_features=3, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (document_embeddings): DocumentRNNEmbeddings(
    (embeddings): StackedEmbeddings(
      (list_embedding_0): WordEmbeddings(
        'glove'
        (embedding): Embedding(400001, 100)
      )
    )
    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)
    (rnn): GRU(256, 512, batch_first=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (weights): None
  (weight_tensor) None
)"
2022-12-19 11:12:13,471 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,472 Corpus: "Corpus: 159999 train + 19999 dev + 19999 test sentences"
2022-12-19 11:12:13,473 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,473 Parameters:
2022-12-19 11:12:13,474  - learning_rate: "0.100000"
2022-12-19 11:12:13,474  - mini_batch_size: "32"
2022-12-19 11:12:13,475  - patience: "8"
2022-12-19 11:12:13,475  - anneal_factor: "0.5"
2022-12-19 11:12:13,475  - max_epochs: "200"
2022-12-19 11:12:13,476  - shuffle: "True"
2022-12-19 11:12:13,476  - train_with_dev: "False"
2022-12-19 11:12:13,477  - batch_growth_annealing: "False"
2022-12-19 11:12:13,477 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,478 Model training base path: "model-saves"
2022-12-19 11:12:13,478 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,480 Device: cpu
2022-12-19 11:12:13,480 ----------------------------------------------------------------------------------------------------
2022-12-19 11:12:13,481 Embeddings storage mode: cpu
2022-12-19 11:12:13,482 ----------------------------------------------------------------------------------------------------
2022-12-19 11:16:30,711 epoch 1 - iter 500/5000 - loss 0.02191805 - samples/sec: 64.52 - lr: 0.100000
2022-12-19 11:20:47,181 epoch 1 - iter 1000/5000 - loss 0.02129665 - samples/sec: 64.78 - lr: 0.100000
2022-12-19 11:25:09,807 epoch 1 - iter 1500/5000 - loss 0.02095840 - samples/sec: 63.33 - lr: 0.100000
2022-12-19 11:29:21,318 epoch 1 - iter 2000/5000 - loss 0.02072234 - samples/sec: 66.26 - lr: 0.100000
2022-12-19 11:33:39,108 epoch 1 - iter 2500/5000 - loss 0.02049498 - samples/sec: 64.36 - lr: 0.100000
2022-12-19 11:37:56,391 epoch 1 - iter 3000/5000 - loss 0.02029193 - samples/sec: 64.66 - lr: 0.100000
2022-12-19 11:42:35,615 epoch 1 - iter 3500/5000 - loss 0.02011083 - samples/sec: 59.44 - lr: 0.100000
2022-12-19 11:47:05,264 epoch 1 - iter 4000/5000 - loss 0.01995082 - samples/sec: 61.71 - lr: 0.100000
2022-12-19 11:51:30,698 epoch 1 - iter 4500/5000 - loss 0.01980279 - samples/sec: 62.65 - lr: 0.100000
2022-12-19 11:56:25,075 epoch 1 - iter 5000/5000 - loss 0.01969604 - samples/sec: 56.61 - lr: 0.100000
2022-12-19 11:56:25,077 ----------------------------------------------------------------------------------------------------
2022-12-19 11:56:25,086 EPOCH 1 done: loss 0.0197 - lr 0.100000
2022-12-19 11:57:35,887 Evaluating as a multi-label problem: False
2022-12-19 11:57:36,102 DEV : loss 0.017413705587387085 - f1-score (micro avg)  0.7099
2022-12-19 11:57:44,821 BAD EPOCHS (no improvement): 0
2022-12-19 11:57:44,831 saving best model
2022-12-19 11:57:46,187 ----------------------------------------------------------------------------------------------------
